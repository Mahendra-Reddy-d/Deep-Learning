{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahendra-Reddy-d/Deep-Learning/blob/main/houseprice%20prediction%20with%20mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "features = [\n",
        "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "    'TotalBsmtSF', 'GrLivArea', 'FullBath', 'HalfBath',\n",
        "    'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea'\n",
        "]\n",
        "target = 'SalePrice'\n",
        "X = train_df[features].fillna(train_df[features].mean())\n",
        "y = train_df[target]\n",
        "X_test_final = test_df[features].fillna(train_df[features].mean())\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test_final)\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=[X_train.shape[1]]),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
        "loss, mae = model.evaluate(X_val, y_val)\n",
        "print(f'Validation MAE: {mae:.2f}')\n",
        "y_test_pred = model.predict(X_test).flatten()\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'SalePrice': y_test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file saved as submission.csv\")\n"
      ],
      "metadata": {
        "id": "4EoeKAxKbfHC",
        "outputId": "c05fba67-3b06-432a-f700-dbfc064b2a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 40206405632.0000 - mae: 183060.7031 - val_loss: 39650586624.0000 - val_mae: 178830.9062\n",
            "Epoch 2/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37820874752.0000 - mae: 180212.8906 - val_loss: 39615942656.0000 - val_mae: 178746.9844\n",
            "Epoch 3/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40698494976.0000 - mae: 185487.8594 - val_loss: 39429767168.0000 - val_mae: 178311.5625\n",
            "Epoch 4/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 38627446784.0000 - mae: 181071.5156 - val_loss: 38785716224.0000 - val_mae: 176828.1875\n",
            "Epoch 5/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 38200360960.0000 - mae: 180359.8906 - val_loss: 37182488576.0000 - val_mae: 173107.8594\n",
            "Epoch 6/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 35223949312.0000 - mae: 173365.7344 - val_loss: 33970573312.0000 - val_mae: 165410.9844\n",
            "Epoch 7/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 31958228992.0000 - mae: 164380.4375 - val_loss: 28677935104.0000 - val_mae: 151790.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27490635776.0000 - mae: 152512.5781 - val_loss: 21678848000.0000 - val_mae: 131419.7188\n",
            "Epoch 9/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19114217472.0000 - mae: 127619.9297 - val_loss: 14250540032.0000 - val_mae: 105481.8828\n",
            "Epoch 10/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12717152256.0000 - mae: 101356.5234 - val_loss: 8493857792.0000 - val_mae: 79619.2188\n",
            "Epoch 11/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8864130048.0000 - mae: 79812.7344 - val_loss: 5797706240.0000 - val_mae: 63170.0820\n",
            "Epoch 12/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7008078848.0000 - mae: 66113.7891 - val_loss: 4950773248.0000 - val_mae: 56807.4062\n",
            "Epoch 13/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6390512640.0000 - mae: 62037.7656 - val_loss: 4646553600.0000 - val_mae: 54392.2695\n",
            "Epoch 14/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5156955136.0000 - mae: 58186.4258 - val_loss: 4399691776.0000 - val_mae: 52518.1562\n",
            "Epoch 15/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5111290880.0000 - mae: 57336.5547 - val_loss: 4236955904.0000 - val_mae: 51637.9453\n",
            "Epoch 16/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6590060544.0000 - mae: 56632.1328 - val_loss: 4049744640.0000 - val_mae: 50353.6641\n",
            "Epoch 17/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4744595456.0000 - mae: 51296.7461 - val_loss: 3847198976.0000 - val_mae: 48818.1914\n",
            "Epoch 18/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5048411136.0000 - mae: 52773.6445 - val_loss: 3670435072.0000 - val_mae: 47588.7031\n",
            "Epoch 19/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4625005056.0000 - mae: 51909.3008 - val_loss: 3521076480.0000 - val_mae: 46726.0898\n",
            "Epoch 20/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4421060608.0000 - mae: 49869.7461 - val_loss: 3351345152.0000 - val_mae: 45495.0273\n",
            "Epoch 21/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4932735488.0000 - mae: 50544.7422 - val_loss: 3204689920.0000 - val_mae: 44416.6406\n",
            "Epoch 22/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3698351872.0000 - mae: 47838.3828 - val_loss: 3066133248.0000 - val_mae: 43413.8008\n",
            "Epoch 23/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4143834624.0000 - mae: 46741.1680 - val_loss: 2944422912.0000 - val_mae: 42523.9727\n",
            "Epoch 24/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3864403200.0000 - mae: 45592.7070 - val_loss: 2810748928.0000 - val_mae: 41398.4297\n",
            "Epoch 25/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3793379840.0000 - mae: 45043.3516 - val_loss: 2709279232.0000 - val_mae: 40641.3789\n",
            "Epoch 26/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3685941504.0000 - mae: 43656.4219 - val_loss: 2592912640.0000 - val_mae: 39579.7695\n",
            "Epoch 27/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3076193024.0000 - mae: 40144.8633 - val_loss: 2489295872.0000 - val_mae: 38600.8906\n",
            "Epoch 28/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3129509120.0000 - mae: 41944.0664 - val_loss: 2401873408.0000 - val_mae: 37855.9961\n",
            "Epoch 29/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3402848512.0000 - mae: 41108.2422 - val_loss: 2354422528.0000 - val_mae: 37402.4297\n",
            "Epoch 30/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2572426240.0000 - mae: 38227.4648 - val_loss: 2266160896.0000 - val_mae: 36520.6016\n",
            "Epoch 31/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3089832448.0000 - mae: 39578.2734 - val_loss: 2184996608.0000 - val_mae: 35674.1367\n",
            "Epoch 32/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2430070016.0000 - mae: 35780.5234 - val_loss: 2122605824.0000 - val_mae: 35026.1211\n",
            "Epoch 33/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3219439104.0000 - mae: 37763.3984 - val_loss: 2087915520.0000 - val_mae: 34565.2266\n",
            "Epoch 34/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2677792512.0000 - mae: 37508.6523 - val_loss: 2022649216.0000 - val_mae: 33903.2578\n",
            "Epoch 35/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2213591552.0000 - mae: 35849.3359 - val_loss: 1957105664.0000 - val_mae: 33270.2656\n",
            "Epoch 36/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2160519680.0000 - mae: 35127.9023 - val_loss: 1936539904.0000 - val_mae: 32910.6875\n",
            "Epoch 37/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2234833664.0000 - mae: 34454.1562 - val_loss: 1903532672.0000 - val_mae: 32472.0488\n",
            "Epoch 38/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2261061632.0000 - mae: 34622.0508 - val_loss: 1888117760.0000 - val_mae: 32172.2051\n",
            "Epoch 39/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2111906560.0000 - mae: 33731.6953 - val_loss: 1828781440.0000 - val_mae: 31565.6855\n",
            "Epoch 40/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2554952448.0000 - mae: 35458.9102 - val_loss: 1794585856.0000 - val_mae: 31144.9766\n",
            "Epoch 41/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1950713600.0000 - mae: 32878.4922 - val_loss: 1758054912.0000 - val_mae: 30709.4355\n",
            "Epoch 42/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1862906112.0000 - mae: 32895.0547 - val_loss: 1726215936.0000 - val_mae: 30336.6309\n",
            "Epoch 43/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2072273920.0000 - mae: 32692.9551 - val_loss: 1727307648.0000 - val_mae: 30171.7402\n",
            "Epoch 44/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1791181824.0000 - mae: 31170.3242 - val_loss: 1682950528.0000 - val_mae: 29734.1641\n",
            "Epoch 45/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2089837952.0000 - mae: 32170.9473 - val_loss: 1677235456.0000 - val_mae: 29529.7871\n",
            "Epoch 46/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1644337408.0000 - mae: 30646.2188 - val_loss: 1653911680.0000 - val_mae: 29252.6055\n",
            "Epoch 47/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1712179840.0000 - mae: 31230.6699 - val_loss: 1635383808.0000 - val_mae: 28975.3398\n",
            "Epoch 48/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1842421120.0000 - mae: 30871.5195 - val_loss: 1642604160.0000 - val_mae: 28894.7090\n",
            "Epoch 49/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1979354752.0000 - mae: 31057.4785 - val_loss: 1616814976.0000 - val_mae: 28576.1406\n",
            "Epoch 50/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2683314944.0000 - mae: 31668.7070 - val_loss: 1591345536.0000 - val_mae: 28289.7559\n",
            "Epoch 51/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1669624064.0000 - mae: 29699.3477 - val_loss: 1556870144.0000 - val_mae: 27929.1309\n",
            "Epoch 52/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1739932800.0000 - mae: 29317.1699 - val_loss: 1604503424.0000 - val_mae: 28126.0078\n",
            "Epoch 53/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1827506944.0000 - mae: 29079.5215 - val_loss: 1571312256.0000 - val_mae: 27825.0273\n",
            "Epoch 54/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2237492480.0000 - mae: 30988.8242 - val_loss: 1550048640.0000 - val_mae: 27589.8535\n",
            "Epoch 55/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1648105984.0000 - mae: 28539.6562 - val_loss: 1538591360.0000 - val_mae: 27450.2871\n",
            "Epoch 56/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1819256320.0000 - mae: 29380.6152 - val_loss: 1529019008.0000 - val_mae: 27277.0664\n",
            "Epoch 57/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2618115584.0000 - mae: 30507.0312 - val_loss: 1517674240.0000 - val_mae: 27122.5352\n",
            "Epoch 58/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1506378240.0000 - mae: 27708.5195 - val_loss: 1505698304.0000 - val_mae: 26983.7676\n",
            "Epoch 59/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1384827520.0000 - mae: 26212.2969 - val_loss: 1513961984.0000 - val_mae: 26949.9961\n",
            "Epoch 60/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1829977728.0000 - mae: 28219.0840 - val_loss: 1520243584.0000 - val_mae: 26896.8398\n",
            "Epoch 61/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1469054464.0000 - mae: 26974.1621 - val_loss: 1497817728.0000 - val_mae: 26659.5371\n",
            "Epoch 62/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1406594816.0000 - mae: 27221.9961 - val_loss: 1480856576.0000 - val_mae: 26484.1309\n",
            "Epoch 63/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1588417792.0000 - mae: 27076.0781 - val_loss: 1498127872.0000 - val_mae: 26492.6055\n",
            "Epoch 64/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1567007616.0000 - mae: 27777.7695 - val_loss: 1505521024.0000 - val_mae: 26464.0117\n",
            "Epoch 65/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1902623744.0000 - mae: 28981.7109 - val_loss: 1465547904.0000 - val_mae: 26212.1113\n",
            "Epoch 66/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1871653248.0000 - mae: 27872.4277 - val_loss: 1458799744.0000 - val_mae: 26073.7246\n",
            "Epoch 67/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1605263232.0000 - mae: 26626.2266 - val_loss: 1454565888.0000 - val_mae: 26010.0840\n",
            "Epoch 68/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1890711808.0000 - mae: 27946.8926 - val_loss: 1437265280.0000 - val_mae: 25795.2598\n",
            "Epoch 69/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1504699136.0000 - mae: 27667.4648 - val_loss: 1424951808.0000 - val_mae: 25703.9336\n",
            "Epoch 70/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1713655680.0000 - mae: 27287.3867 - val_loss: 1449905280.0000 - val_mae: 25685.3730\n",
            "Epoch 71/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1629451776.0000 - mae: 27650.4375 - val_loss: 1437395584.0000 - val_mae: 25563.5859\n",
            "Epoch 72/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1794768256.0000 - mae: 26757.5684 - val_loss: 1424052096.0000 - val_mae: 25444.6484\n",
            "Epoch 73/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1876986496.0000 - mae: 27247.4395 - val_loss: 1404225280.0000 - val_mae: 25332.6035\n",
            "Epoch 74/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1439851904.0000 - mae: 26828.1836 - val_loss: 1403441792.0000 - val_mae: 25198.7832\n",
            "Epoch 75/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2207736320.0000 - mae: 27908.2266 - val_loss: 1392140032.0000 - val_mae: 25140.4512\n",
            "Epoch 76/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1200387968.0000 - mae: 25277.8984 - val_loss: 1376896256.0000 - val_mae: 25008.4551\n",
            "Epoch 77/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1365323648.0000 - mae: 26275.9531 - val_loss: 1389099648.0000 - val_mae: 24971.4551\n",
            "Epoch 78/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1726019968.0000 - mae: 26723.4707 - val_loss: 1380628224.0000 - val_mae: 24932.0918\n",
            "Epoch 79/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1623058048.0000 - mae: 25643.3848 - val_loss: 1393719040.0000 - val_mae: 24881.7500\n",
            "Epoch 80/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1610023680.0000 - mae: 26219.5449 - val_loss: 1358174720.0000 - val_mae: 24737.1191\n",
            "Epoch 81/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1789520896.0000 - mae: 26872.5547 - val_loss: 1386912384.0000 - val_mae: 24783.9062\n",
            "Epoch 82/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1410577920.0000 - mae: 26217.3945 - val_loss: 1381257984.0000 - val_mae: 24705.4102\n",
            "Epoch 83/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1620696576.0000 - mae: 26656.3594 - val_loss: 1373852160.0000 - val_mae: 24598.8594\n",
            "Epoch 84/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1544393216.0000 - mae: 26025.3711 - val_loss: 1360328320.0000 - val_mae: 24511.8613\n",
            "Epoch 85/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1510815104.0000 - mae: 25777.3613 - val_loss: 1372147072.0000 - val_mae: 24501.6406\n",
            "Epoch 86/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1612557440.0000 - mae: 27045.9922 - val_loss: 1348916608.0000 - val_mae: 24385.6289\n",
            "Epoch 87/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1306515072.0000 - mae: 25464.5801 - val_loss: 1327752960.0000 - val_mae: 24305.0762\n",
            "Epoch 88/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1237885952.0000 - mae: 26527.9238 - val_loss: 1318537856.0000 - val_mae: 24213.0996\n",
            "Epoch 89/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1364691072.0000 - mae: 25594.5156 - val_loss: 1331456256.0000 - val_mae: 24147.0215\n",
            "Epoch 90/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1694177408.0000 - mae: 25983.7168 - val_loss: 1328752000.0000 - val_mae: 24152.2148\n",
            "Epoch 91/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1281906816.0000 - mae: 24383.5547 - val_loss: 1307917824.0000 - val_mae: 24117.0918\n",
            "Epoch 92/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1216690048.0000 - mae: 24619.5977 - val_loss: 1322691712.0000 - val_mae: 24064.4219\n",
            "Epoch 93/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1407677056.0000 - mae: 24670.7949 - val_loss: 1361515776.0000 - val_mae: 24146.4531\n",
            "Epoch 94/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1334443520.0000 - mae: 24552.5684 - val_loss: 1318804224.0000 - val_mae: 23950.3027\n",
            "Epoch 95/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1330092160.0000 - mae: 24422.4785 - val_loss: 1334513792.0000 - val_mae: 23959.1973\n",
            "Epoch 96/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1232894976.0000 - mae: 24166.6680 - val_loss: 1356036864.0000 - val_mae: 24008.6875\n",
            "Epoch 97/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2351873536.0000 - mae: 26686.4512 - val_loss: 1324058880.0000 - val_mae: 23900.0820\n",
            "Epoch 98/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1783397248.0000 - mae: 25157.1152 - val_loss: 1322453504.0000 - val_mae: 23885.6855\n",
            "Epoch 99/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1428873216.0000 - mae: 24945.5391 - val_loss: 1335007360.0000 - val_mae: 23841.8340\n",
            "Epoch 100/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1547311616.0000 - mae: 25360.7344 - val_loss: 1294832128.0000 - val_mae: 23734.0059\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1158424064.0000 - mae: 21765.8984\n",
            "Validation MAE: 23734.01\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load training and test data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Step 2: Select feature columns (numeric features, edit as needed)\n",
        "features = [\n",
        "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "    'TotalBsmtSF', 'GrLivArea', 'FullBath', 'HalfBath',\n",
        "    'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea'\n",
        "]\n",
        "target = 'SalePrice'\n",
        "\n",
        "# Step 3: Handle missing values in train and test features\n",
        "X = train_df[features].fillna(train_df[features].mean())\n",
        "y = train_df[target]\n",
        "X_test_final = test_df[features].fillna(train_df[features].mean())\n",
        "\n",
        "# Step 4: Split train data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Scale numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test_final)\n",
        "\n",
        "# Step 6: Build the MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=[X_train.shape[1]]),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 7: Train model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Step 8: Evaluate on validation set\n",
        "loss, mae = model.evaluate(X_val, y_val)\n",
        "print(f'Validation MAE: {mae:.2f}')\n",
        "\n",
        "# Step 9: Make predictions for test set and display sample output\n",
        "y_test_pred = model.predict(X_test).flatten()\n",
        "print(\"Sample predicted prices on test set:\", y_test_pred[:5])\n",
        "\n",
        "# Step 10: Prepare submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'SalePrice': y_test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file saved as submission.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CMRQ5saKfJyK",
        "outputId": "697f151b-0eaa-4c43-9fa0-781b09c9f6c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 38039928832.0000 - mae: 180292.6406 - val_loss: 39649165312.0000 - val_mae: 178827.1719\n",
            "Epoch 2/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38244560896.0000 - mae: 180883.8438 - val_loss: 39609233408.0000 - val_mae: 178731.7344\n",
            "Epoch 3/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 38390767616.0000 - mae: 180444.7031 - val_loss: 39413194752.0000 - val_mae: 178284.3906\n",
            "Epoch 4/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39613894656.0000 - mae: 182447.0781 - val_loss: 38780170240.0000 - val_mae: 176860.4844\n",
            "Epoch 5/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35906719744.0000 - mae: 174995.6094 - val_loss: 37228908544.0000 - val_mae: 173374.0156\n",
            "Epoch 6/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35304980480.0000 - mae: 174321.4688 - val_loss: 34205794304.0000 - val_mae: 166345.2969\n",
            "Epoch 7/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 32640065536.0000 - mae: 165082.2188 - val_loss: 29322412032.0000 - val_mae: 154167.5156\n",
            "Epoch 8/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27615252480.0000 - mae: 152991.6875 - val_loss: 22780979200.0000 - val_mae: 135872.2500\n",
            "Epoch 9/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22254057472.0000 - mae: 136270.5938 - val_loss: 15502088192.0000 - val_mae: 111498.4453\n",
            "Epoch 10/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13900149760.0000 - mae: 108309.6016 - val_loss: 9775129600.0000 - val_mae: 86953.4062\n",
            "Epoch 11/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9896325120.0000 - mae: 86579.9453 - val_loss: 6529247232.0000 - val_mae: 68375.9688\n",
            "Epoch 12/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7758652928.0000 - mae: 70772.4609 - val_loss: 5286069248.0000 - val_mae: 59488.7734\n",
            "Epoch 13/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6312871424.0000 - mae: 63310.0977 - val_loss: 4853046784.0000 - val_mae: 55980.2109\n",
            "Epoch 14/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6116887040.0000 - mae: 60462.4922 - val_loss: 4622073856.0000 - val_mae: 54265.8633\n",
            "Epoch 15/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5118729728.0000 - mae: 57527.0859 - val_loss: 4382682112.0000 - val_mae: 52358.5195\n",
            "Epoch 16/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5112403968.0000 - mae: 57679.1484 - val_loss: 4203228928.0000 - val_mae: 51198.3438\n",
            "Epoch 17/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5037042176.0000 - mae: 54316.4453 - val_loss: 4005815296.0000 - val_mae: 49894.1758\n",
            "Epoch 18/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5600064000.0000 - mae: 54186.7383 - val_loss: 3856338432.0000 - val_mae: 48968.2305\n",
            "Epoch 19/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4376229376.0000 - mae: 51782.0664 - val_loss: 3676530432.0000 - val_mae: 47699.4648\n",
            "Epoch 20/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5079954944.0000 - mae: 53181.4766 - val_loss: 3529018112.0000 - val_mae: 46712.8477\n",
            "Epoch 21/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6218413056.0000 - mae: 53081.2773 - val_loss: 3354428416.0000 - val_mae: 45458.8633\n",
            "Epoch 22/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3733397760.0000 - mae: 48334.4648 - val_loss: 3182836224.0000 - val_mae: 44094.3086\n",
            "Epoch 23/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3834487040.0000 - mae: 47533.8398 - val_loss: 3109344768.0000 - val_mae: 43743.2109\n",
            "Epoch 24/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3669322752.0000 - mae: 46631.2812 - val_loss: 2950949376.0000 - val_mae: 42493.4219\n",
            "Epoch 25/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3340150784.0000 - mae: 44162.7305 - val_loss: 2841078784.0000 - val_mae: 41568.4141\n",
            "Epoch 26/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2931178496.0000 - mae: 42231.8125 - val_loss: 2687723776.0000 - val_mae: 40230.5117\n",
            "Epoch 27/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3215766528.0000 - mae: 42651.1250 - val_loss: 2610776832.0000 - val_mae: 39616.4570\n",
            "Epoch 28/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3483748864.0000 - mae: 41923.8281 - val_loss: 2506293248.0000 - val_mae: 38664.8945\n",
            "Epoch 29/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3167898624.0000 - mae: 41284.2148 - val_loss: 2389210624.0000 - val_mae: 37568.9570\n",
            "Epoch 30/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3539229440.0000 - mae: 40932.0195 - val_loss: 2330840832.0000 - val_mae: 37032.9141\n",
            "Epoch 31/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2897654784.0000 - mae: 40232.6133 - val_loss: 2248200192.0000 - val_mae: 36225.1055\n",
            "Epoch 32/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4036944384.0000 - mae: 39895.2383 - val_loss: 2177354240.0000 - val_mae: 35555.5547\n",
            "Epoch 33/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2295791104.0000 - mae: 36338.4102 - val_loss: 2100383872.0000 - val_mae: 34816.3477\n",
            "Epoch 34/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2537475584.0000 - mae: 36751.6680 - val_loss: 2070491648.0000 - val_mae: 34461.5742\n",
            "Epoch 35/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2470362112.0000 - mae: 35189.4219 - val_loss: 2019165056.0000 - val_mae: 33889.8984\n",
            "Epoch 36/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2599971072.0000 - mae: 35777.9727 - val_loss: 1955529856.0000 - val_mae: 33266.1406\n",
            "Epoch 37/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2245040896.0000 - mae: 34464.6758 - val_loss: 1917794048.0000 - val_mae: 32835.2656\n",
            "Epoch 38/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2147545600.0000 - mae: 34782.0898 - val_loss: 1881024768.0000 - val_mae: 32416.2578\n",
            "Epoch 39/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2372730880.0000 - mae: 34952.0117 - val_loss: 1869920896.0000 - val_mae: 32176.2871\n",
            "Epoch 40/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2105543168.0000 - mae: 34695.0078 - val_loss: 1834611584.0000 - val_mae: 31727.4492\n",
            "Epoch 41/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2381509120.0000 - mae: 34440.1875 - val_loss: 1803416064.0000 - val_mae: 31307.6543\n",
            "Epoch 42/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2055610496.0000 - mae: 33362.3672 - val_loss: 1746423424.0000 - val_mae: 30778.3359\n",
            "Epoch 43/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2070466688.0000 - mae: 32984.7422 - val_loss: 1746842880.0000 - val_mae: 30662.4238\n",
            "Epoch 44/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2379464704.0000 - mae: 33413.0312 - val_loss: 1706034304.0000 - val_mae: 30246.8828\n",
            "Epoch 45/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1850246912.0000 - mae: 31296.6680 - val_loss: 1676136320.0000 - val_mae: 29917.6055\n",
            "Epoch 46/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2081989760.0000 - mae: 32301.3379 - val_loss: 1661454976.0000 - val_mae: 29721.4922\n",
            "Epoch 47/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1834851968.0000 - mae: 31128.1660 - val_loss: 1661192448.0000 - val_mae: 29586.3906\n",
            "Epoch 48/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1662141440.0000 - mae: 30617.0098 - val_loss: 1625944448.0000 - val_mae: 29206.1465\n",
            "Epoch 49/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2151901952.0000 - mae: 31386.4473 - val_loss: 1646679808.0000 - val_mae: 29258.3770\n",
            "Epoch 50/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2184510720.0000 - mae: 32138.9062 - val_loss: 1604471296.0000 - val_mae: 28815.2773\n",
            "Epoch 51/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1776799488.0000 - mae: 30308.7305 - val_loss: 1592416512.0000 - val_mae: 28618.1504\n",
            "Epoch 52/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1525197440.0000 - mae: 28691.7930 - val_loss: 1560340352.0000 - val_mae: 28249.8789\n",
            "Epoch 53/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1665708160.0000 - mae: 28525.5586 - val_loss: 1561940352.0000 - val_mae: 28175.2402\n",
            "Epoch 54/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1994494976.0000 - mae: 30839.1719 - val_loss: 1568638336.0000 - val_mae: 28122.2871\n",
            "Epoch 55/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1946972288.0000 - mae: 30649.6680 - val_loss: 1557826176.0000 - val_mae: 27894.9980\n",
            "Epoch 56/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1884063360.0000 - mae: 29907.1348 - val_loss: 1525624832.0000 - val_mae: 27577.3867\n",
            "Epoch 57/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2109542784.0000 - mae: 29849.4961 - val_loss: 1529539968.0000 - val_mae: 27509.1328\n",
            "Epoch 58/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1813178880.0000 - mae: 29141.6973 - val_loss: 1533538176.0000 - val_mae: 27393.9023\n",
            "Epoch 59/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1840929536.0000 - mae: 27332.6562 - val_loss: 1494622464.0000 - val_mae: 27051.2363\n",
            "Epoch 60/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1594919680.0000 - mae: 28073.3086 - val_loss: 1500605952.0000 - val_mae: 27048.3672\n",
            "Epoch 61/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1585217664.0000 - mae: 27999.1582 - val_loss: 1499736320.0000 - val_mae: 26907.1953\n",
            "Epoch 62/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1495473536.0000 - mae: 27750.1582 - val_loss: 1492203904.0000 - val_mae: 26777.4512\n",
            "Epoch 63/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1405794688.0000 - mae: 27072.3535 - val_loss: 1477244160.0000 - val_mae: 26591.2129\n",
            "Epoch 64/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1360118656.0000 - mae: 26185.0020 - val_loss: 1470164992.0000 - val_mae: 26430.3984\n",
            "Epoch 65/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1818512256.0000 - mae: 28849.3008 - val_loss: 1463472768.0000 - val_mae: 26302.2422\n",
            "Epoch 66/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1842988672.0000 - mae: 27972.3750 - val_loss: 1445542144.0000 - val_mae: 26151.2754\n",
            "Epoch 67/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1385112704.0000 - mae: 27152.4004 - val_loss: 1424751360.0000 - val_mae: 25942.9375\n",
            "Epoch 68/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1427808896.0000 - mae: 26431.1309 - val_loss: 1443900160.0000 - val_mae: 25940.6836\n",
            "Epoch 69/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1610674560.0000 - mae: 27427.3438 - val_loss: 1435754112.0000 - val_mae: 25814.0586\n",
            "Epoch 70/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1335176320.0000 - mae: 25857.6250 - val_loss: 1408643072.0000 - val_mae: 25605.8848\n",
            "Epoch 71/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1725062016.0000 - mae: 26739.6914 - val_loss: 1432064896.0000 - val_mae: 25644.5801\n",
            "Epoch 72/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1483185664.0000 - mae: 26903.2578 - val_loss: 1395045248.0000 - val_mae: 25399.7070\n",
            "Epoch 73/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1457137408.0000 - mae: 26545.1973 - val_loss: 1410216448.0000 - val_mae: 25389.7656\n",
            "Epoch 74/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1781546112.0000 - mae: 27302.9238 - val_loss: 1392716928.0000 - val_mae: 25231.4453\n",
            "Epoch 75/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1638185088.0000 - mae: 27347.4258 - val_loss: 1390341248.0000 - val_mae: 25126.8066\n",
            "Epoch 76/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2104512512.0000 - mae: 27029.7344 - val_loss: 1365408896.0000 - val_mae: 24990.1758\n",
            "Epoch 77/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1799962112.0000 - mae: 26976.4180 - val_loss: 1358057216.0000 - val_mae: 24897.5195\n",
            "Epoch 78/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1377512704.0000 - mae: 26016.6426 - val_loss: 1367269760.0000 - val_mae: 24851.2773\n",
            "Epoch 79/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1420560000.0000 - mae: 25738.7500 - val_loss: 1378710016.0000 - val_mae: 24833.0488\n",
            "Epoch 80/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1639622528.0000 - mae: 26453.3047 - val_loss: 1346044544.0000 - val_mae: 24701.4883\n",
            "Epoch 81/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1797480064.0000 - mae: 26113.9824 - val_loss: 1370135168.0000 - val_mae: 24725.4258\n",
            "Epoch 82/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1499076096.0000 - mae: 25433.9258 - val_loss: 1350435456.0000 - val_mae: 24632.6777\n",
            "Epoch 83/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1490140800.0000 - mae: 25273.0547 - val_loss: 1344979328.0000 - val_mae: 24539.7051\n",
            "Epoch 84/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1606858112.0000 - mae: 26810.7266 - val_loss: 1355315328.0000 - val_mae: 24522.7148\n",
            "Epoch 85/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1715837184.0000 - mae: 25042.2910 - val_loss: 1367253888.0000 - val_mae: 24503.9434\n",
            "Epoch 86/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1117482624.0000 - mae: 24348.8496 - val_loss: 1312611072.0000 - val_mae: 24331.8262\n",
            "Epoch 87/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1407940864.0000 - mae: 24180.6035 - val_loss: 1390573312.0000 - val_mae: 24541.9980\n",
            "Epoch 88/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1576051072.0000 - mae: 25973.4766 - val_loss: 1345033088.0000 - val_mae: 24374.5039\n",
            "Epoch 89/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1198489344.0000 - mae: 25067.2617 - val_loss: 1331977344.0000 - val_mae: 24260.2305\n",
            "Epoch 90/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1401172864.0000 - mae: 24336.3301 - val_loss: 1352497664.0000 - val_mae: 24281.7012\n",
            "Epoch 91/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1824944896.0000 - mae: 26398.6133 - val_loss: 1335604224.0000 - val_mae: 24182.2148\n",
            "Epoch 92/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1323855104.0000 - mae: 24502.6016 - val_loss: 1327780480.0000 - val_mae: 24121.2285\n",
            "Epoch 93/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1289094656.0000 - mae: 24137.4805 - val_loss: 1328838144.0000 - val_mae: 24098.7422\n",
            "Epoch 94/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1541996160.0000 - mae: 26281.9531 - val_loss: 1323364352.0000 - val_mae: 24027.5430\n",
            "Epoch 95/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1819262592.0000 - mae: 26324.9805 - val_loss: 1324529408.0000 - val_mae: 23982.1289\n",
            "Epoch 96/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1477979264.0000 - mae: 25088.8164 - val_loss: 1346717440.0000 - val_mae: 24036.5215\n",
            "Epoch 97/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1365365760.0000 - mae: 25256.7227 - val_loss: 1300356608.0000 - val_mae: 23928.4629\n",
            "Epoch 98/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1706086016.0000 - mae: 27278.6602 - val_loss: 1322023808.0000 - val_mae: 23887.8086\n",
            "Epoch 99/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1096068736.0000 - mae: 23742.9453 - val_loss: 1282534784.0000 - val_mae: 23779.1816\n",
            "Epoch 100/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1811927168.0000 - mae: 26031.0625 - val_loss: 1306529408.0000 - val_mae: 23797.2832\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1174831360.0000 - mae: 21809.4805\n",
            "Validation MAE: 23797.28\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Sample predicted prices on test set: [110440.17 141650.95 162299.77 191506.12 213286.03]\n",
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function for single input prediction ---\n",
        "\n",
        "def predict_price(input_dict):\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "    for feat in features:\n",
        "        if feat not in input_df.columns:\n",
        "            input_df[feat] = train_df[feat].mean()\n",
        "    input_df = input_df[features]\n",
        "    input_df = input_df.fillna(train_df[features].mean())\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    pred_price = float(model.predict(input_scaled)[0])\n",
        "    return pred_price\n",
        "\n",
        "example_input = {\n",
        "    'LotArea': 8450,\n",
        "    'OverallQual': 7,\n",
        "    'OverallCond': 5,\n",
        "    'YearBuilt': 2003,\n",
        "    'YearRemodAdd': 2003,\n",
        "    'TotalBsmtSF': 856,\n",
        "    'GrLivArea': 1710,\n",
        "    'FullBath': 2,\n",
        "    'HalfBath': 1,\n",
        "    'BedroomAbvGr': 3,\n",
        "    'TotRmsAbvGrd': 8,\n",
        "    'GarageCars': 2,\n",
        "    'GarageArea': 548\n",
        "}\n",
        "\n",
        "predicted_price = predict_price(example_input)\n",
        "print(f\"Predicted house price for example input: ${predicted_price:.2f}\")\n"
      ],
      "metadata": {
        "id": "EdJjMyD1f2BB",
        "outputId": "b5fb8e02-0d37-4a1e-abdd-a9872e158e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Predicted house price for example input: $212669.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1095597795.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  pred_price = float(model.predict(input_scaled)[0])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}