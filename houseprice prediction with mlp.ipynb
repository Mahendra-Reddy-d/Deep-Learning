{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahendra-Reddy-d/Deep-Learning/blob/main/houseprice%20prediction%20with%20mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#dataset\n",
        "#https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
      ],
      "metadata": {
        "id": "ioefEi3at5Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load training data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Step 2: Select feature columns and target\n",
        "features = [\n",
        "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "    'TotalBsmtSF', 'GrLivArea', 'FullBath', 'HalfBath',\n",
        "    'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea'\n",
        "]\n",
        "target = 'SalePrice'\n",
        "\n",
        "# Step 3: Handle missing values in features\n",
        "X = train_df[features].fillna(train_df[features].mean())\n",
        "y = train_df[target]\n",
        "\n",
        "# Step 4: Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Step 6: Build the MLP model with Input Layer\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Step 8: Predict on validation set\n",
        "y_val_pred = model.predict(X_val).flatten()\n",
        "\n",
        "# Step 9: Calculate accuracy metrics on validation set\n",
        "mae = mean_absolute_error(y_val, y_val_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f'Validation MAE: {mae:.2f}')\n",
        "print(f'Validation RMSE: {rmse:.2f}')\n",
        "\n",
        "# Step 10: Show some sample predictions vs actual\n",
        "print(\"\\nSample predictions on validation data:\")\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_val.iloc[i]}, Predicted: {y_val_pred[i]:.2f}\")"
      ],
      "metadata": {
        "id": "wF0mbGXJt77Q",
        "outputId": "3dddcc06-8b07-4d51-eeef-e3148f9fcee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 38394527744.0000 - mae: 180214.0156 - val_loss: 39650807808.0000 - val_mae: 178831.1719\n",
            "Epoch 2/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37422944256.0000 - mae: 178475.6094 - val_loss: 39620378624.0000 - val_mae: 178753.2656\n",
            "Epoch 3/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39430545408.0000 - mae: 181740.6875 - val_loss: 39452512256.0000 - val_mae: 178338.6406\n",
            "Epoch 4/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38005395456.0000 - mae: 179802.7656 - val_loss: 38848417792.0000 - val_mae: 176870.3906\n",
            "Epoch 5/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40982011904.0000 - mae: 184363.2344 - val_loss: 37347160064.0000 - val_mae: 173198.8281\n",
            "Epoch 6/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34273179648.0000 - mae: 170757.8125 - val_loss: 34276161536.0000 - val_mae: 165508.2656\n",
            "Epoch 7/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 32145506304.0000 - mae: 163819.0938 - val_loss: 29268852736.0000 - val_mae: 152085.1719\n",
            "Epoch 8/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27202824192.0000 - mae: 151407.9219 - val_loss: 22410137600.0000 - val_mae: 131731.2812\n",
            "Epoch 9/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20481792000.0000 - mae: 130250.1328 - val_loss: 15032333312.0000 - val_mae: 106639.8906\n",
            "Epoch 10/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13867199488.0000 - mae: 104891.8203 - val_loss: 9183458304.0000 - val_mae: 81646.7500\n",
            "Epoch 11/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8929766400.0000 - mae: 82163.5703 - val_loss: 6164130304.0000 - val_mae: 64820.5742\n",
            "Epoch 12/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6743490560.0000 - mae: 66948.3438 - val_loss: 5136356352.0000 - val_mae: 57750.4727\n",
            "Epoch 13/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6952964608.0000 - mae: 61794.7070 - val_loss: 4717230080.0000 - val_mae: 54510.0820\n",
            "Epoch 14/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7251294720.0000 - mae: 60530.5039 - val_loss: 4433050624.0000 - val_mae: 52487.1992\n",
            "Epoch 15/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4784651776.0000 - mae: 55523.7227 - val_loss: 4205101824.0000 - val_mae: 50953.5977\n",
            "Epoch 16/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5307295232.0000 - mae: 57782.4883 - val_loss: 4007604992.0000 - val_mae: 49700.5000\n",
            "Epoch 17/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5557689344.0000 - mae: 54500.8242 - val_loss: 3810864128.0000 - val_mae: 48311.4336\n",
            "Epoch 18/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4933931520.0000 - mae: 53832.1641 - val_loss: 3628282880.0000 - val_mae: 47173.4805\n",
            "Epoch 19/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3867795968.0000 - mae: 49555.4023 - val_loss: 3463523328.0000 - val_mae: 46051.7148\n",
            "Epoch 20/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5960261632.0000 - mae: 52994.2383 - val_loss: 3302320640.0000 - val_mae: 44948.2617\n",
            "Epoch 21/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4805367808.0000 - mae: 49119.3359 - val_loss: 3126891008.0000 - val_mae: 43509.9258\n",
            "Epoch 22/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3996145152.0000 - mae: 46974.2930 - val_loss: 2989552896.0000 - val_mae: 42605.9102\n",
            "Epoch 23/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4192621568.0000 - mae: 46044.9766 - val_loss: 2855877888.0000 - val_mae: 41578.9844\n",
            "Epoch 24/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3310071808.0000 - mae: 45080.2500 - val_loss: 2725210624.0000 - val_mae: 40594.6523\n",
            "Epoch 25/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4296660480.0000 - mae: 44126.9102 - val_loss: 2626565888.0000 - val_mae: 39801.0273\n",
            "Epoch 26/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3088077056.0000 - mae: 42932.9102 - val_loss: 2526895104.0000 - val_mae: 38895.6172\n",
            "Epoch 27/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2937074944.0000 - mae: 39845.9727 - val_loss: 2451071744.0000 - val_mae: 38268.0898\n",
            "Epoch 28/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3518463488.0000 - mae: 41163.4648 - val_loss: 2346022144.0000 - val_mae: 37278.8750\n",
            "Epoch 29/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2838594560.0000 - mae: 38824.6211 - val_loss: 2277734400.0000 - val_mae: 36584.5781\n",
            "Epoch 30/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2854718720.0000 - mae: 39432.2383 - val_loss: 2236617472.0000 - val_mae: 36127.5898\n",
            "Epoch 31/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2421128192.0000 - mae: 36724.6758 - val_loss: 2150235392.0000 - val_mae: 35263.0586\n",
            "Epoch 32/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2504108032.0000 - mae: 37964.1758 - val_loss: 2070563456.0000 - val_mae: 34436.3984\n",
            "Epoch 33/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2521934848.0000 - mae: 35267.4453 - val_loss: 2011535488.0000 - val_mae: 33759.8281\n",
            "Epoch 34/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2658134528.0000 - mae: 36631.6875 - val_loss: 1942312448.0000 - val_mae: 33056.9453\n",
            "Epoch 35/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2023683968.0000 - mae: 33466.2930 - val_loss: 1936198528.0000 - val_mae: 32763.3770\n",
            "Epoch 36/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2032512256.0000 - mae: 33694.2031 - val_loss: 1897190272.0000 - val_mae: 32313.4727\n",
            "Epoch 37/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2273034240.0000 - mae: 35263.0273 - val_loss: 1851685376.0000 - val_mae: 31813.4961\n",
            "Epoch 38/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2312434944.0000 - mae: 34258.3438 - val_loss: 1856947968.0000 - val_mae: 31618.6602\n",
            "Epoch 39/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1898480896.0000 - mae: 31996.8809 - val_loss: 1807280512.0000 - val_mae: 31097.6328\n",
            "Epoch 40/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2457792256.0000 - mae: 33836.4531 - val_loss: 1764624896.0000 - val_mae: 30597.0098\n",
            "Epoch 41/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1985583360.0000 - mae: 32214.7695 - val_loss: 1750195968.0000 - val_mae: 30329.9766\n",
            "Epoch 42/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3270392832.0000 - mae: 36143.0742 - val_loss: 1697231488.0000 - val_mae: 29807.5586\n",
            "Epoch 43/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1852599680.0000 - mae: 30999.1367 - val_loss: 1680518656.0000 - val_mae: 29579.0898\n",
            "Epoch 44/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1907172992.0000 - mae: 31687.9824 - val_loss: 1672356096.0000 - val_mae: 29376.7402\n",
            "Epoch 45/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1970106880.0000 - mae: 30873.5684 - val_loss: 1644732160.0000 - val_mae: 29067.0098\n",
            "Epoch 46/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1775863040.0000 - mae: 30497.1973 - val_loss: 1610617728.0000 - val_mae: 28703.3711\n",
            "Epoch 47/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1767925120.0000 - mae: 30052.4668 - val_loss: 1632279424.0000 - val_mae: 28722.1465\n",
            "Epoch 48/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1894060800.0000 - mae: 29825.3906 - val_loss: 1622231296.0000 - val_mae: 28520.9766\n",
            "Epoch 49/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2191953152.0000 - mae: 32298.4199 - val_loss: 1611205632.0000 - val_mae: 28309.8906\n",
            "Epoch 50/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1556729088.0000 - mae: 29295.9355 - val_loss: 1569594112.0000 - val_mae: 27911.1270\n",
            "Epoch 51/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2292594432.0000 - mae: 31173.5723 - val_loss: 1604792448.0000 - val_mae: 28007.7090\n",
            "Epoch 52/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1621440128.0000 - mae: 29933.6914 - val_loss: 1555679872.0000 - val_mae: 27619.7383\n",
            "Epoch 53/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1868622720.0000 - mae: 29465.9297 - val_loss: 1561676672.0000 - val_mae: 27566.2188\n",
            "Epoch 54/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1641135232.0000 - mae: 28283.1836 - val_loss: 1549177856.0000 - val_mae: 27419.8340\n",
            "Epoch 55/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1611754624.0000 - mae: 28333.2832 - val_loss: 1527620864.0000 - val_mae: 27220.3008\n",
            "Epoch 56/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1682419712.0000 - mae: 29921.2402 - val_loss: 1516963328.0000 - val_mae: 27027.4355\n",
            "Epoch 57/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1751163904.0000 - mae: 29146.9492 - val_loss: 1552793600.0000 - val_mae: 27186.5684\n",
            "Epoch 58/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1684784128.0000 - mae: 27958.4492 - val_loss: 1516596480.0000 - val_mae: 26902.1719\n",
            "Epoch 59/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2079911552.0000 - mae: 29620.0449 - val_loss: 1518852480.0000 - val_mae: 26782.3848\n",
            "Epoch 60/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1517547264.0000 - mae: 27043.1211 - val_loss: 1500891648.0000 - val_mae: 26604.7637\n",
            "Epoch 61/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1389802752.0000 - mae: 26904.5195 - val_loss: 1478067072.0000 - val_mae: 26451.7031\n",
            "Epoch 62/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1419885696.0000 - mae: 27482.5918 - val_loss: 1456551296.0000 - val_mae: 26219.1602\n",
            "Epoch 63/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1341270912.0000 - mae: 26437.7539 - val_loss: 1522894464.0000 - val_mae: 26544.1484\n",
            "Epoch 64/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1743774976.0000 - mae: 28026.7246 - val_loss: 1505946496.0000 - val_mae: 26362.0762\n",
            "Epoch 65/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1381814912.0000 - mae: 25602.9102 - val_loss: 1486819328.0000 - val_mae: 26190.7617\n",
            "Epoch 66/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1743167488.0000 - mae: 27515.9258 - val_loss: 1467360640.0000 - val_mae: 26012.7715\n",
            "Epoch 67/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1554283648.0000 - mae: 27204.1035 - val_loss: 1465949184.0000 - val_mae: 25940.8262\n",
            "Epoch 68/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1330069248.0000 - mae: 26190.7051 - val_loss: 1443999488.0000 - val_mae: 25778.8086\n",
            "Epoch 69/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2274210816.0000 - mae: 29352.9453 - val_loss: 1455350272.0000 - val_mae: 25781.4062\n",
            "Epoch 70/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1383769472.0000 - mae: 26344.3242 - val_loss: 1401725952.0000 - val_mae: 25486.5781\n",
            "Epoch 71/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1511678208.0000 - mae: 26081.9688 - val_loss: 1427031040.0000 - val_mae: 25504.4473\n",
            "Epoch 72/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1434670848.0000 - mae: 25262.1621 - val_loss: 1417980032.0000 - val_mae: 25400.2285\n",
            "Epoch 73/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1606429824.0000 - mae: 26276.0859 - val_loss: 1411478912.0000 - val_mae: 25322.0254\n",
            "Epoch 74/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1643401472.0000 - mae: 26175.7695 - val_loss: 1407417856.0000 - val_mae: 25236.3652\n",
            "Epoch 75/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1147719040.0000 - mae: 25081.9863 - val_loss: 1389795584.0000 - val_mae: 25106.1094\n",
            "Epoch 76/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1639952896.0000 - mae: 26976.1777 - val_loss: 1467230976.0000 - val_mae: 25405.1738\n",
            "Epoch 77/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1428288512.0000 - mae: 26164.0859 - val_loss: 1408212352.0000 - val_mae: 25126.4824\n",
            "Epoch 78/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1807735680.0000 - mae: 26625.8496 - val_loss: 1407591552.0000 - val_mae: 25054.9473\n",
            "Epoch 79/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1482819968.0000 - mae: 25607.9160 - val_loss: 1383589760.0000 - val_mae: 24881.3906\n",
            "Epoch 80/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1737499520.0000 - mae: 26347.9863 - val_loss: 1369420800.0000 - val_mae: 24799.2676\n",
            "Epoch 81/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1545784704.0000 - mae: 26171.5137 - val_loss: 1362082688.0000 - val_mae: 24702.0898\n",
            "Epoch 82/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1451295872.0000 - mae: 25669.3613 - val_loss: 1368010240.0000 - val_mae: 24712.0684\n",
            "Epoch 83/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1300012032.0000 - mae: 25048.3750 - val_loss: 1386004992.0000 - val_mae: 24630.2422\n",
            "Epoch 84/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1319783296.0000 - mae: 26112.9297 - val_loss: 1363258112.0000 - val_mae: 24542.3184\n",
            "Epoch 85/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1545634304.0000 - mae: 25298.6562 - val_loss: 1396065408.0000 - val_mae: 24586.2012\n",
            "Epoch 86/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1316563072.0000 - mae: 24853.0840 - val_loss: 1359410816.0000 - val_mae: 24440.7129\n",
            "Epoch 87/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1515572096.0000 - mae: 25811.1797 - val_loss: 1338799616.0000 - val_mae: 24362.4609\n",
            "Epoch 88/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1520420992.0000 - mae: 26310.5879 - val_loss: 1350241280.0000 - val_mae: 24281.8828\n",
            "Epoch 89/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1387142144.0000 - mae: 24960.4473 - val_loss: 1362800128.0000 - val_mae: 24314.4492\n",
            "Epoch 90/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1469331328.0000 - mae: 25415.1777 - val_loss: 1370100608.0000 - val_mae: 24285.7324\n",
            "Epoch 91/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1367061888.0000 - mae: 26218.4375 - val_loss: 1363902080.0000 - val_mae: 24211.9570\n",
            "Epoch 92/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2007530880.0000 - mae: 27089.7832 - val_loss: 1336657152.0000 - val_mae: 24127.9414\n",
            "Epoch 93/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1117141120.0000 - mae: 24015.9238 - val_loss: 1316060032.0000 - val_mae: 24007.2695\n",
            "Epoch 94/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1645089408.0000 - mae: 25596.1484 - val_loss: 1364258432.0000 - val_mae: 24101.9102\n",
            "Epoch 95/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1457499648.0000 - mae: 24901.0020 - val_loss: 1331967360.0000 - val_mae: 23997.0625\n",
            "Epoch 96/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1696033920.0000 - mae: 26075.1797 - val_loss: 1336743808.0000 - val_mae: 23958.6172\n",
            "Epoch 97/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1567218304.0000 - mae: 25805.7344 - val_loss: 1330555008.0000 - val_mae: 23931.4512\n",
            "Epoch 98/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1391777536.0000 - mae: 25128.5293 - val_loss: 1317587200.0000 - val_mae: 23836.4297\n",
            "Epoch 99/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1526496000.0000 - mae: 25578.0117 - val_loss: 1306777344.0000 - val_mae: 23766.4062\n",
            "Epoch 100/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1356533888.0000 - mae: 25073.3887 - val_loss: 1297838208.0000 - val_mae: 23675.5605\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Validation MAE: 23675.56\n",
            "Validation RMSE: 36025.52\n",
            "\n",
            "Sample predictions on validation data:\n",
            "Actual: 154500, Predicted: 147876.98\n",
            "Actual: 325000, Predicted: 282750.72\n",
            "Actual: 115000, Predicted: 127185.91\n",
            "Actual: 159000, Predicted: 150164.17\n",
            "Actual: 315500, Predicted: 312966.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Function to predict price from new input ---\n",
        "def predict_price(input_dict):\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "    for feat in features:\n",
        "        if feat not in input_df.columns:\n",
        "            input_df[feat] = train_df[feat].mean()\n",
        "    input_df = input_df[features]\n",
        "    input_df = input_df.fillna(train_df[features].mean())\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    pred_array = model.predict(input_scaled)\n",
        "    pred_price = pred_array[0, 0]  # Extract scalar properly\n",
        "    return pred_price\n",
        "\n",
        "# Example usage of prediction function\n",
        "example_input = {\n",
        "    'LotArea': 8450,\n",
        "    'OverallQual': 7,\n",
        "    'OverallCond': 5,\n",
        "    'YearBuilt': 2003,\n",
        "    'YearRemodAdd': 2003,\n",
        "    'TotalBsmtSF': 856,\n",
        "    'GrLivArea': 1710,\n",
        "    'FullBath': 2,\n",
        "    'HalfBath': 1,\n",
        "    'BedroomAbvGr': 3,\n",
        "    'TotRmsAbvGrd': 8,\n",
        "    'GarageCars': 2,\n",
        "    'GarageArea': 548\n",
        "}\n",
        "\n",
        "predicted_price = predict_price(example_input)\n",
        "print(f\"\\nPredicted house price for example input: ${predicted_price:.2f}\")"
      ],
      "metadata": {
        "id": "dAg3tXODuN1Y",
        "outputId": "c5ab1629-01ea-4c6a-d5e9-2f7550deb19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\n",
            "Predicted house price for example input: $211897.86\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}